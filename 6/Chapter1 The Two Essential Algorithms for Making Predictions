Chapter 1 The Two Essential Algorithms for Making Predictions 

This book focuses on the machine learning process and so covers just a few of the most effective and widely used algorithms. It does not provide a survey if machien learning techniques. Too many of the algorithms that might be included in a suvey are nit actively used by practitioners. 
This book deals with one class of machine learning problems, generally referred to as function approximation. Function approximation is a sunset of problems that are called supervised learning problems. Linear regression and its classifier cousin, logistic regression, provide familiar examples of algorithms for function approximation problems.Function approximation problems include an enormous breadth of practical classification and regression problems in all sorts of arenas, including text classification, search responses, ad placements, spam filtering, predicting customer behavior, diagnostics, and so forth. The list is almost endless. 
Broadly speaking, this book covers two classes of algorithms for soving function approximation problems: penalized linear regression methods and ensemble methods. This chapter introduces you to both of these algorithms, outlines some of their characteristics, and reviews the results of comparative studies of algorithm performance in order to demonstrate their consistent high performance. 
This chapter then discusses the process of building predictive models. It describes the kinds of problem that you'll be able to address with the tool covered here and the flexibilities that you hace in how you set up your problem and define the features that you'll use for making predictions. It describes process steps involved in building a predictive model and qualifying it for deployment. 

Why Are These Two Algorithms So Useful?
----------------------------------------
Several factors make the penalized linear regression and ensemble methods a useful collection. Stated simply,they will provide optinum or near-optimum performance on the vast majority of predictive analytics(function approximation) problems encountered in practice, includeing big data sets, little data sets, wide data sets, tall skinny data sets, complicated problems, and simple problems.Evidence for this assertion can be found in two papers by Rich Caruana and his colleagues:
·   "An Empirical Comparison of Supervised Learning Algorithms," by Rich Caruana and Alexandru Niculescu-Mizil 
·   "An Empirical Evaluation of Supervised Learning in High Dimensions," by Rich Caruana, Nikos Karampatziakits, and Ainur Yessenalina. 
In those two papers, the authors chose a variety of classification problems and applied a variety of different algorithms to build predictive models.The models were run on test data that were not included in training the models, and then the algorithms included in the studies were ranked on the basis of their performance on the problems.The first study compared 9 different basic algorithms on 11 different machine learning(binary classification) problems. 
The problems used in the study came from a wide variety of areas, including demographic data, text processing, pattern recognition, physics, and biology. Table 1-1 lists the data sets used in the study using the same names given by the study authors. The table shows how many attributes were available for predicting outcomes for each of the data sets, and it show s what percentage of the examples were positive. 
The term positive example in a classification problem means an experment(a lien of data from the input data set) in which the outcome is positive. For example, if the classifier is being designed to determine whether a radar retun sihnal indicates the presence of an airplane, then the positive example would be those returns where there was actually an airplane in the radar's field of view. The tern positive comes from this sort of example where the two outcomes represent presence or absence. Other examples include presence or absence of disease in a medical test or presence or absence of cheating on a tax return. 
Not all classification problems deal with presence or absence. For example, determing the gender of an author by machine-reading their text or machine-analyzing a handwriting sample has two classes-male and female--but there's no sense in which one is the absence of the other.In these cases, these's some arbitrariness in the assignment of the designations "positive" and "negative". The assignments of positive and negative can be arbitrary, but once chosen must be uesd consistently.
Some of the problems in the first stuby had many more examples of one class than the other. These are called unbalanced. For example, the two data sets Letter.p1 and Letter.p2 pose closely related problems in correctly classifying typed uppercase letters in a wide variety of fonts. The task with Letter.p1 is to correctly classify the letter O in a standard mix of letters. The task with Letter.p2 is to correctly classify A-M versus N-Z. The percentage of positives shown in Table 1-1 reflects this didderence.
Table 1-1 also shows the number of "attributes" in each of the data sets. Attributes are the variables you have available to base a prediction on. For example, to predict whether an airplane will arrive at its destination on time or not, you might incorporate attributes such as the name of the airline company, the make and year of the airplane,the level of precipitation at the destination airport, the wind speed and direction along the flight path, and so on. Having a lot of attributes upon which to base a prediction can be a blessing and a curse.Attributes that relate directly to the outcomes being predicted are a blessing. Attributes that are unrelated to the outcomes are a curse. Telling the difference between blesses and cursed attributes requires data. Chapter 3,"Predictive Model Building: Balancing Performance, Complexity, and Big Data," goes into that in more detail. 

Table 1-1: Sketch of Problems in Machine Learning Comparison Study 
    -----------------------------------------------------------------------------
    DATA SET NAME | NUMBER OF ATTRIBUTES  |  % OF EXAMPLES THAT ARE POSITIVE   
    --------------|-----------------------|--------------------------------------
    Adult         | 14                    |  25  
    Bact          | 11                    | 69 
    COd           | 15                    | 50 
    Calhous       | 9                     | 52 
    Cov_Type      | 54                    | 36 
    HS            | 200                   | 24 
    Letter.p1     | 16                    | 3 
    Letter.p2     | 16                    | 53 
    Medis         | 63                    | 11 
    Mg            | 124                   | 17 
    Slac          | 59                    | 50 
    ----------------------------------------------------------------------------

Table 1-2 shows how the algorithms covered in this book fared relative to the other algorithms used in the study. Table 1-2 shows which algorithms showed the top five performance scores for ech of the problems listed in Table 1-1. Algorithms covered in this book are spelled out(boosted decision trees, Random Forest, Bagged decision trees, and logisitic regression). The first therr of these are ensemble methods. Penalized regression was not fully developed when the study was done and wasn't evaluated. Logisitc regression is a close relative and is used to gauge the success of regression methods. Each of the 9 algorithms used in the study had 3 different data reduction techniques applied, for a total of 27 combinations.The top five positions represent roughly the top 20 percent of performance scores.The row next to the heading Covt indicates that the boosted decision trees algorithm was was the first and second best relative to performance, Random Forests algorirhm was the fourth and fifth best, and bagged decision trees algorithm was the third best. In the cases where algorithms nit covered here were in the top five, an entry appears in the Other column. The algorithms that show up there are k nearest neighbors(KNNs), artificial neural nets(ANNs),and support vector machines(SVMs).

Table 1-2 How the Algorithms Coverd in This Book Compare on Different Problems 
    ------------------------------------------------------------------------------------------------------------
    ALGORITHM | BOOSTED DECISION TREES | RANDOM FORESTS | BAGGED DECISION TREES | LOGISTIC REGRESSION | OTHER
    ----------|------------------------|----------------|-----------------------|---------------------|---------
    Cort      | 1,2                    | 4,5            | 3
    Adult     | 1,4                    | 2              | 3,5                    
    LTR.P1    | 1                      |                |                       |                     | SVM,KNN 
    LTR.P2    | 1,2                    | 4,5            |                       |                     | SVM 
    MEDIS     |                        | 1,3            |                       | 5                   | ANN 
    SLAC      |                        | 1,2,3          | 4,5 
    HS        | 1,3                    |                |                       |                     | ANN 
    MG        |                        | 2,4,5          | 1,3 
    CALHOUS   | 1,2                    | 5              | 3,4 
    COD       | 1,2                    |                | 3,4,5 
    BACT      | 2,5                    |                | 1,3,4
    ------------------------------------------------------------------------------------------------------------

Logistic regression captures top-five honors in only ine case in Table 1-2. The reason for that is that these data sets have few attributes(at most 200) relative to example(5000 in each data set).These's plenty of data to resolve a model with so few attributes, and yet the training  sets are small enough that the training time is not execssive. 

[NOTE]  As you'll see in Chapter 3 and in the examples covered in Chapter 5,"Building Predictive Models Using Penalized Linear Methods," and Chapter 7,"Building Ensemble Models with Python," the penalized regression methods perform best relative to other algorithms when there are numerous attributes and not enough examples or time to train a more complicated ensemble model. 

Caruana et al.have run a newer study(2008) to address how these algorithms compare when the number of attributes increases. That is, how do these algorithms compare on big data? A numberof fields have significantly more attributes than the data sets in the first study.For example, genomic problems have several tens of thousands of attributes(one attribute per gene), and text mining problems can have millions of attributes(one attribute per distinct word or per distinct pair of words).Table 1-3 shows how linear regression and ensemble methods fare as the number of attributes grows.The results in Table 1-3 show the ranking of the algorithms used in the second study.The table shows the performance on each of the problems individually and in the far right column shows the ranking of each algorithm's average score across all the problems.The algorithms uesd in the study are broken into two groups. The top group of algorithms are ones that will be covered in this book.The bottom group will not be covered. 
The problems shown in Table 1-3 are arranged in order of their number if attributes, ranging from 761 to 685569.Linear(logistic) regression is in the top three for 5 of the 11 test cases uesd in the study.Those superior scores were concentrated among the larger data sets.Notice that boosted decision tree(denoted by BSTDT in Table 1-3) and Random Forest (denoted by RF in Table 1-3) algorithms still perform near the top.They come in first and second for overall score on these problems. 
The algorithms covered in this book have other advantages besides raw predictive performance.An important benefit of the penalized linear regression models that the book covers is the speed at which they train. On big problems, training speed can become an issue. In some problems, model training can take days or weeks. This time frame can be an intolerable delay, particularly early in development when iterations are required to home in on the best approach. Besides training very quickly, after being deployed a trained linear model can produce predictions very quickly--quickly enough for high-speed trading or Internet ad insertions. The study demonstrates that penalized linear regression can provide the best answers available in many cases and be near the top even in cases where they are not the best. 
In addition, these algorithms are reasionably easy to use.They do not have very many tunable parameters. They have well-defined and well-structured imput types. They solve several types of problems in regression and classification. It is nit unusual to be able to arrange the imput data and generate a first trained model and performance predictions within an hour or two of starting a new problem. 

Table 1-3: How the Algorithms Covered in This Book Compare on Big Data Problems 
    ------------------------------------------------------------------------------------
         |761  |761  |780   |927|1344 |3448 |20958|105354|195203|405333|685569|
    DIM  |STURN|CALAM|DIGITS|TIS|CRYST|KDD98|R-S  |CITE  |DSE   |SPAM  |IMDB  |MEAN 
    -----|-----|-----|------|---|-----|-----|-----|------|------|------|------|---------
    BSTDT|8    |1    |2     |6  |1    |3    |8    |1     |7     |6     |3     |1
    RF   |9    |4    |3     |3  |2    |1    |6    |5     |3     |1     |3     |2 
    BAGDT|5    |2    |6     |4  |3    |1    |9    |1     |6     |7     |3     |4
    BSTST|2    |3    |7     |7  |7    |1    |7    |4     |8     |8     |5     |7
    LR   |4    |8    |9     |1  |4    |1    |2    |2     |2     |4     |4     |6
    SVM  |3    |5    |5     |2  |5    |2    |1    |1     |5     |5     |3     |3
    ANN  |6    |7    |4     |5  |8    |1    |4    |2     |1     |3     |3     |5
    KNN  |1    |6    |1     |9  |6    |2    |10   |1     |7     |9     |6     |8
    PRC  |7    |9    |8     |8  |7    |1    |3    |3     |4     |2     |2     |9
    NB   |10   |10   |10    |10 |9    |1    |5    |1     |9     |10    |7     |10
    -------------------------------------------------------------------------------------

one of their most omportant features is that they indicate which of their imput variables is most important for producing predictions. This turns out to be an invaluable feature in a machine learning algorithm. One of the most time-consuming steps in the development of a predictive model is what is sometimes called feature selection or feature engineering. This is the process whereby the data scientist choose the variables that will be used to predict outcomes. By ranking features according to importance, the algorithms covered in this book aid in the feature-engineering process by takine some of the guesswork out of the development process and making the process more sure. 

What Are Penalized Regression Methods? 
---------------------------------------

Penalized linear regression is a derivative of ordinary least squares(OLS) regression--a method developed by Gauss and Legendre roughly 200 year age. Penalized linear regression methods were designed to overcome some basic limitations of OLS regression. The basic problem with OLS is that sometimes it overfits the problem. Think of OLS as fitting a line through a group of points, as in Figure a single attribute x. For example, the problem might be to predict men's salaries using only their heights. Height is slightly predictive of salaries for men(but not for women).

Figure 1-1: Ordinary least squares fit 

The points represent men's salaries versus their heights. The line in Figure 1-1 represents the OLS solution to this prediction problem.Insome sense, the line si the best predictive model for men's salaries given their hrights. The data set has six points in it. Suppose that data set had only two point in it. Imagine that there's a population of points, like the ones in Figure 1-1, but that you do not get to see all the point. Maybe they are too expensive to generate, like the genetic data mentioned earlier. There are enough humans available to isolate the gene that is the culprit; the problem is that you do not have gene sequences for many of them because of cost. 
To simulate this the siple example, imagine that instead of six points you're given only two of six points. How would that change the nature of the line fit to those points? It would depend on which two points you happened to get. To see how much effect that would have,pick any two points from Figure 1-1 and imagine a line through them. Figure 1-2 shows some of the possible lines through pairs of points from Figure 1-1. Notice how much the lines vary depending on the choice of points. 

Figure 1-2: Fitting lines with only two points 

The problem with having only to points to fit a line is that there is not enough data for the number of defrees of freedom. A line has two degrees of freedom. Having two degrees of freedom means that there are two independent parameters that uniquely determine a line.You can imagine grabbing hold of a line in the plane and sliding it up and down in the plane or twisting it to changed seqartely, and together they completely specify a line. The degrees of freedom of a line can be expressed in serveral equivalent ways(where it intercepts the y-axis and its slope, two points that are on the line, and so on).All of these representations of a line require two parameters to specify. 
When the number of degrees of greedom is equal to the number of points, the predictions are not very good. The lines hit the points used to draw them, but there is a lot of variation among lines drawn with different pairs of points.You cannot place much faith in a prediction that has as many degrees if freedom as the number of points in your data set. The plot in Figure 1-1 had six points and fit a line(two degrees of freedom) through them. That is six points and two degrees of freedom. The through problem of determining the genes causing a heritable condition illustrated that having more genes to choose from makes it necessary to have more data in order to isolate a cause from among the 20000 or so possible human genes.The 20000 different genes represent 20000 degrees of greedom. Data from even 20000 different persons will not suffice to get a reliable answer, and in many cases, all that can be afforded within the scope of a reasonable study is a sample from 500 or so persons. That is where penalized linear regression may be the best algorithm choice. 
Penalized linear regression provides a way to systematically reduce degrees of freedom to mutch the amount of data available and the complexity of the underlying phenomena. These methods have become very popular for problems with very many degrees of freedom. They are a favorite for genetic problems where the number of degrees of freedom(that is, the number of genes) can be several tens of thousand and for problems line text calssification where the number of degrees of freedom can be more than a million. Chapter 4,"Penalized Linear Regression," gives more detail on how there methods work, sample code that illustrates the mechanics of there algorithms, and examples of the process for implementing machine learning systems using available Python packages. 

What Are Ensemble Methods? 
---------------------------

The other family of algorithms covered in this book is ensemble methods. The basic idea with ensmeble methods is to build a horde of different predicative models and then combine their outputs--by averaging the outputs or taking the majority answer(voting). The individual models are called base learners. Some results from computational learning theory show that if the base learners are just slightly better than random guessing, the performance of the ensemble can be very good if there is a sufficient number of independent models. 
One of the problems spurring the development of ensemnle methods has been the observation that some particular machine learning algorithms exhibit instability. For example, the addition of fresh data to the data set might result in a radical channge in the resulting model or its performance. Binary decision trees and traditional neural nets exhibit this sort of instability. This instability causes high variance in the performance of models, and averaging many models can be viewed as a way to reduce the variance. The trick is how to generate large 
numbers of independent models, particularly if they are all using the same base learner. Chapter 6, "Ensemble Methods," will get into the details of how this is done. The techniques are ingenious, and it is relatively easy to understand their basic principles of operation. Here is a preview of what's in store. 
The ensemble methods that enjoy the widest availability and usage incorporate binary decision trees as their base learners. Binary decision trees are often portrayed as shown in Figure 1-3. The tree in Figure 1-3 takes a real number, called x, as imput at the top, and then uses a series of binary(two-valued) decisions to decide what value should be output in response to x. The first decision is whether x is less than 5. If the answer to that question is "no," the binary decision tree outputs the value 4 indicated in the circle below the No leg of the upper decision box. Every possible value for x leads to some output y from the tree. Figure 1-4 plots the output(y) as a function of the input to the tree (x). 

Figure 1-3: Binary decision tree example 

Figure 1-4: Input-output graph for the bianry decision tree example 

This description raises the question of where the comparisons (for example, x<5?) come from and where the output values (in the circles at the bottom of the tree) come from. These values come from training the binary tree on the input data. The algorithm for doing that training is not diffecult to understand and is covered in Chapter 6.The important thing to note at this point is that the values in the trained binary decision tree are fixed,given the data.The process for generating the tree is training data and train on these random subsets. That technique is called Bagging(short for bootstrap aggregating). It gives a way ti generate a large number of slightly different binary decision trees.Those are then averaged (or voted for a classifier) to yield a final result. Chapter 6 describes in more detail this technique and other more powerful ones. 

How to Decide Which Algorithm to Use 
-------------------------------------

Table 1-4 gives a sketch comparison of these two families of algorithms. Penalized linear regression methods have the advantage that they train very quickly. Training times on large data sets can extend to hours, days, or even weeks. Training usually needs to be done several times before a deployable solution is arrived at. Long training times can stall development and deployment on large problems. The rapid training time for penalized linear methods makes them useful for the obvious reason that shorter is better.Depending on the problem, these methods may suffer some performance disadvantages relative to ensemble methods. Chapter 3 gives more insight into the types of problems where penalized regression might be a better choise and those where ensemble methods might be a better choise.Penalized linear methods can sometimes be a useful first step in your development process even in the circumstance where they yield infermance to ensemble methods. 
Early in development, a number of training iterations will be necessary for purposes of feature selection and feature engineering and for solidifying the mathematical problem statement. Deciding what you are going to use as imput to your predictive model can take some time and thought.Sometiems that is obvious, but usually it requires some iteration. Throwing in everything you can find is not usually a good solution. 
Trial and error is typically required to determine the best imputs for a model. For example, if you're trying to predict whether a visitor. Maybe that does not give you the accuracy that you need, so you try incorating data regarding the visitor's past behavior on the site--what ad the visitor clicked during past site visits or what products the visitor has bought. Maybe adding data about the site the visitor was on before coming to your site would help.These questions lead to a series of experiments where you incorporate the new data and see whether it huits or helps. This iteration is generally time-consuming both for the data manipulations and for training your predictive model.Penalized linear regression will generally be faster than an ensemble method, and the time difference can be a material factor in the development process. 
For example, if the training set is on the order of a gigabyte, training times may be on the order of 30 minutes for penalized linear regression and 5 or 6 hours for an ensemble method. If the feature engineering process requires 10 iterations to select the best feature set, the computation time alone comes to the difference between taking a day or taking a week to accomplish feature engineering. A useful process, therefore, is to train a penalized linear model in the early stages of development, feature engineering, and so on.That gives the data scientist a feel which variables are going to be useful and important as well as a baseline performance for comparison with other algorithms later in development. 
